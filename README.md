## SwissPDGs-TimeSeries

In this project, we aim to generate time series for the Swiss power distribution grids data set. The time series demand profile for each grid is generated through the following processes. Since industrial load data is missing, we only consider commercial and residential loads in this project:

1. **Percentage Assignment**: The demand composition of each node in each LV grid is calculated using Voronoi partitioning based on the load percentage data from the `x_el_dmd.json` file.
2. **Standard Profile Generation**: The standard 24-hour residential and commercial profiles for each municipality are generated by calculating the weighted average of the residential and commercial profiles.
3. **Combination**: The time series profile of a node is generated by multiplying the load percentage of the node by the standard profiles of the municipality in which the node is located:

```math
\text{profile} = \text{res\_percentage} \times \text{res\_profile} + \text{com\_percentage} \times \text{com\_profile}
```

Next, each process and the assumptions made will be explained in detail.

### Percentage Assignment

There are a total of 2,148 municipalities, each with an `el_dmd.json` profile that provides load percentage data for each small area within the municipality. Additionally, we have grid data for each municipality, where each node in the grids represents the aggregated buildings in a small area. Our task is to assign a load percentage to each node. The code to perform this task is in `allocation.py`.

- Fill empty percentage values in `el_dmd.json` with 0.
- Each `el_dmd.json` file provides load percentages at certain representative load points. The number of load points is denoted by \(a\).
  - For \(a > 2\), we use Voronoi partitioning to divide the municipality based on the locations of each point. We add auxiliary nodes without load information at the periphery, far enough from the original points, to close all the partitioned areas. We then assign the load percentage of each point to the nodes in its partitioned area. For nodes located outside all partitioned areas, the load percentage of the nearest load point is assigned.
  - For \(a \leq 2\), the points are insufficient for Voronoi partitioning. In this case, we simply assign each node in the grid the load percentage of the nearest load point.
- After the allocation, the residential percentage and commercial percentage of each node is saved in the original grid JSON file.

### Standard Profile Generation

- We categorized demand profiles by demand source and their construction period. The table below shows the categories. Then we counted the number of each type of demand in each municipality and computed weighted average demand curves accordingly. It is worth mentioning that this demand is 8760 hours, while we need to calculate the profile of millions of nodes, which is a big data storage and computational challenge, so in the next step, we calculated representative 24-hour profiles from this yearly profile.

| Load Type     | Residential                | Commercial                                      |
|---------------|----------------------------|-------------------------------------------------|
| Building Type | Single-family house        | School, Hospital, Offices, Shops, Restaurants   |
| Period        | Before 1920, 1920-1945, 1946-1960, 1961-1970, 1971-1980, 1981-1985, 1986-1990, 1991-1995, 1996-2000, 2001-2005, 2006-2010, 2011-2015 | Before 1920, 1920-1945, 1946-1960, 1961-1970, 1971-1980, 1981-1985, 1986-1990, 1991-1995, 1996-2000, 2001-2005, 2006-2010, 2011-2015 |

- After generating a yearly residential and commercial profile for each municipality, we further processed the yearly profile to get a representative daily profile to reduce data redundancy.
  - For the commercial demand profile, since it has significant seasonal differences, we use k-means to cluster the 365 daily profiles into several clusters. The number of clusters is determined by choosing the best silhouette score. 
  - For the residential demand profile, it doesn't have significant seasonal differences. We calculated an average daily profile using the yearly profile.
- Demand profiles for some municipalities are missing, we fill the missing profiles with average profile of other municipalities.
- Finally, the daily residential and commercial profiles are normalized to the range 0-1.

### Combination

The results derived in step 1 and step 2 are combined together, and the demand profile in each node can be calculated by the following formula:

```math
\text{profile} = \text{res\_percentage} \times \text{res\_profile} + \text{com\_percentage} \times \text{com\_profile}
```

Here is an example of how to generate the demand profile for a grid. You can copy the code below or run the code in `exemple.ipynb`.

```python
import pickle
import pandas as pd
import numpy as np
import geopandas as gpd
import os
import tqdm
import json
import warnings
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')

grid_id = '2'
# read folder dictionary
with open('data_processing/dict_folder.json') as f:
    folders = json.load(f)
# read the profile
with open('municipality_profiles/commercial_profile_k-means.pkl', 'rb') as f:
    commercial_profile_kmean = pickle.load(f)
residential_profile = pd.read_csv('municipality_profiles/residential_profiles_24h.csv', index_col=0)
commercial_profile = pd.read_csv('municipality_profiles/commercial_profiles_24h.csv', index_col=0)
commercial_profile_day = commercial_profile_kmean[grid_id]['profile'].reshape(-1, 24)
counts = commercial_profile_kmean[grid_id]['counts']
repeated_profiles = [profile for profile, count in zip(commercial_profile_day, counts) for _ in range(count)]
np.random.shuffle(repeated_profiles)
commercial_profile_year = np.concatenate(repeated_profiles)
residential_profile_day = np.array(residential_profile[grid_id])
residential_profile_year = np.tile(residential_profile_day, 365)
```

```python
path = 'LV/' + folders[grid_id] + '/'
grid_ids = list(set([str(f.split('.')[0][:-6]) for f in os.listdir(path) if f.startswith(grid_id + '-')]))
print("Processing grid {} in municipality {}".format(grid_id, folders[grid_id]))
for n in tqdm.tqdm(range(len(grid_ids))):
    i = grid_ids[n]
    node_id = i + "_nodes"
    nodes = gpd.read_file('LV/' + folders[grid_id] + '/' + node_id)
    nodes['profile'] = {}
    for iter, row in nodes.iterrows():
        combined_profile = row['res_percentage'] * residential_profile_year + row['com_percentage'] * commercial_profile_year
        nodes.at[iter, 'profile'] = combined_profile
    # create folder LV_with_profiles if it does not exist
    if not os.path.exists('LV_with_profiles/' + folders[grid_id]):
        os.makedirs('LV_with_profiles/' + folders[grid_id])
    save_path = 'LV_with_profiles/' + folders[grid_id] + '/' + i + '_nodes'
    nodes.to_pickle(save_path)
```
### Directory Structure

```plaintext
.
├── data_processing
│   └── [Scripts and data for preprocessing raw input data]
├── Demand_calculator
│   └── [Scripts for calculating electrical demand based on various profiles and parameters]
├── LV
│   └── [Low-voltage network data, including node and connection information]
├── municipality_profiles
│   └── [Standard profiles for different municipalities]
│   ├── commercial_profile_k-means.pkl
│   │     └── [Serialized k-means clustering model for typical daily commercial profiles]
│   ├── commercial_profiles_24h.csv
│   │     └── [CSV file with average daily commercial profiles over a year]
│   ├── residential_profiles_24h.csv
│         └── [CSV file with average daily residential profiles over a year]
├── Square_zones_dmd
│   └── [Data and scripts for analyzing demand in square zones]
├── allocation.py
│   └── [Script for percentage-based allocation processes]
├── exemple.ipynb
│   └── [Example Jupyter notebook demonstrating how to generate demand profiles for each node in a grid]
├── profile_assignment.py
│   └── [Script for identifying typical days in a year and assigning profiles to nodes within the low-voltage network. Use `exemple.ipynb` for generating profiles for a single grid rather than a batch process]
```

