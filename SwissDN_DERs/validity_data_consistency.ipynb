{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import is_object_dtype\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrity and logical consistency of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value types and shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected output types are:\n",
    "- 16 x 3 string columns: \n",
    "    - 1 x 3 for LV_P_installed.csv\n",
    "    - 1 x 3 for LV_generation.csv\n",
    "    - 1 x 3 for LV_std.csv\n",
    "    - 1 x 3 for MV_P_installed.csv\n",
    "    - 1 x 3 for MV_generation.csv\n",
    "    - 1 x 3 for MV_std.csv\n",
    "    - 1 x 3 for BESS_allocation_LV.csv\n",
    "    - 1 x 3 for BESS_allocation_MV.csv\n",
    "    - 2 x 3 for LV_heat_pump_allocation.csv\n",
    "    - 2 x 3 for MV_heat_pump_allocation.csv\n",
    "    - 1 x 3 for EV_allocation.csv\n",
    "    - 1 x 3 for EV_power_profiles.csv\n",
    "    - 1 x 3 for LV_basicload_shares.csv.\n",
    "\n",
    "The other columns are numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the name of the files \n",
    "folders = os.listdir()\n",
    "folders = [folder for folder in folders if '.' not in folder] # we remove the files from the list\n",
    "# files per folder\n",
    "years = ['2030', '2040', '2050']\n",
    "files = []\n",
    "for folder in folders:\n",
    "    for year in years:\n",
    "        # we get the name of the files \n",
    "        files_list = os.listdir(os.path.join(folder, year))\n",
    "        files += [os.path.join(folder, year, file) for file in files_list if file.endswith('.csv')]\n",
    "# we read the files \n",
    "dfs = {}\n",
    "for file in files:\n",
    "    # we read the file\n",
    "    df = pd.read_csv(file)\n",
    "    # we get the columns with object type\n",
    "    cols = [col for col in df.columns if is_object_dtype(df[col])]\n",
    "    # we convert the columns to string type\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('string')\n",
    "    der, year, type_file = file.split('\\\\')\n",
    "    # we add the dataframe to the dictionary\n",
    "    dfs[(der, year, type_file)] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the types of the dataframe\n",
    "cols, dtypes = [], []\n",
    "der_list, year_list, type_file_list = [], [], []\n",
    "for key, df_temp in dfs.items():\n",
    "    der, year, type_file = key\n",
    "    # we get the name of the columns\n",
    "    cols += df_temp.columns.tolist()\n",
    "    # we get the types of the columns\n",
    "    dtypes += [str(t) for t in df_temp.dtypes.tolist()]\n",
    "    # we get the identifiers of the file\n",
    "    der_list += [der] * len(df_temp.columns)\n",
    "    year_list += [year] * len(df_temp.columns)\n",
    "    type_file_list += [type_file] * len(df_temp.columns)\n",
    "# we create a dataframe with the types of the columns\n",
    "df_types = pd.DataFrame({'column': cols, 'type': dtypes, 'der': der_list, 'year': year_list, 'type_file': type_file_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The identifiers are correct\n"
     ]
    }
   ],
   "source": [
    "# we check the non float64 and int64 columns\n",
    "# the string types must be 16 * 3 \n",
    "if df_types[~df_types['type'].isin(['float64', 'int64'])].shape[0] == 16 * 3:\n",
    "    print('The identifiers are correct')\n",
    "else:\n",
    "    print('ERROR: The identifiers are not correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numeric columns are correct\n"
     ]
    }
   ],
   "source": [
    "# we check if the other columns are numeric\n",
    "if (df_types.shape[0] - df_types[df_types['type'].isin(['float64', 'int64'])].shape[0]) == df_types[~df_types['type'].isin(['float64', 'int64'])].shape[0]:\n",
    "    print('The numeric columns are correct')\n",
    "else:\n",
    "    print('ERROR: The numeric columns are not correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we print the shape of the dataframe to check if the expected number of columns is correct\n",
    "der_list, year_list, type_list, rows_list, column_list = [], [], [], [], []\n",
    "for key, df_temp in dfs.items():\n",
    "    der, year, type_file = key\n",
    "    # we get the shape of the dataframe\n",
    "    der_list.append(der)\n",
    "    year_list.append(year)\n",
    "    type_list.append(type_file)\n",
    "    rows_list.append(df_temp.shape[0])\n",
    "    column_list.append(df_temp.shape[1])\n",
    "# we create a dataframe with the shape of the dataframes\n",
    "df_shape = pd.DataFrame({'der': der_list, 'year': year_list, 'type_file': type_list, 'columns': column_list, 'rows': rows_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct unique number of columns are (as shown in the Supplementary material): 1, 3, 4, 6, 7, 53, 290, 366, 8761, 8762.\n",
    "\n",
    "Total unique number of columns: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 6, 7, 53, 290, 366, 8761, 8762]\n",
      "Unique number of columns:  10\n"
     ]
    }
   ],
   "source": [
    "print(sorted(df_shape['columns'].unique())) # we check the number of columns\n",
    "print('Unique number of columns: ', len(df_shape['columns'].unique())) # we check the number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct unique number of rows are (as shown in the Supplementary material): 440, 2148, 3625, 6444, 7498, 8760, 11202, 11551, 13726, 13784, 14759, 15429, 19452, 151034, 384288, 481318, 630171, 758118, 876594, 998977, 1065403, 1427096, 2525530\n",
    "\n",
    "Total unique number of rows: 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[440, 2148, 3625, 6444, 7498, 8760, 11202, 11551, 13726, 13784, 14759, 15429, 19452, 151034, 384288, 481318, 630171, 758118, 876594, 998977, 1065403, 1427096, 2525530]\n",
      "Unique number of rows:  23\n"
     ]
    }
   ],
   "source": [
    "print(sorted(df_shape['rows'].unique()))\n",
    "print('Unique number of rows: ', len(df_shape['rows'].unique())) # we check the number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## None values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files have no none values\n"
     ]
    }
   ],
   "source": [
    "# we check if there are none values\n",
    "none_values = False\n",
    "for key, df_temp in dfs.items():\n",
    "    der, year, type_file = key\n",
    "    # we check if there are none values\n",
    "    if df_temp.isnull().sum().sum() > 0:\n",
    "        none_values = True\n",
    "        print(f'ERROR: The file {key} has none values')\n",
    "if not(none_values):\n",
    "    print('The files have no none values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency at the micro-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PV generation should not be greater than the installed power rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LV installed PV power is equal or greater than the generation at every node for year 2030: True\n",
      "MV installed PV power is equal or greater than the generation at every node for year 2030: True \n",
      "\n",
      "LV installed PV power is equal or greater than the generation at every node for year 2040: True\n",
      "MV installed PV power is equal or greater than the generation at every node for year 2040: True \n",
      "\n",
      "LV installed PV power is equal or greater than the generation at every node for year 2050: True\n",
      "MV installed PV power is equal or greater than the generation at every node for year 2050: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    # TODO: delete the multiplication by 0.001 when the dataframes are updated\n",
    "    LV_installed_geq_gen = (dfs['01_PV', year, 'LV_generation.csv'].iloc[:,2:].max(axis=1) * 0.001 <= dfs['01_PV', year, 'LV_P_installed.csv']['P_installed_kW']).all()\n",
    "    print(f'LV installed PV power is equal or greater than the generation at every node for year {year}: {LV_installed_geq_gen}')\n",
    "    MV_installed_geq_gen = (dfs['01_PV', year, 'MV_generation.csv'].iloc[:,2:].max(axis=1) * 0.001 <= dfs['01_PV', year, 'MV_P_installed.csv']['P_installed_kW']).all()\n",
    "    print(f'MV installed PV power is equal or greater than the generation at every node for year {year}: {MV_installed_geq_gen}', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the consistency of the EV profiles.\n",
    "- We verify if the base charging is equal or lower than the upper charging profile. Also, we verify that the lower charging profile is equal or lower than the base profile.\n",
    "- We confirm that the flexible energy per day is equal or lower than the sum of the maximum absolute deviation between the lower/uppder profile with the base during the days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upper profile is valid for year 2030: True\n",
      "The lower profile is valid for year 2030: True\n",
      "The flexibility is valid for year 2030: True \n",
      "\n",
      "The upper profile is valid for year 2040: True\n",
      "The lower profile is valid for year 2040: True\n",
      "The flexibility is valid for year 2040: True \n",
      "\n",
      "The upper profile is valid for year 2050: True\n",
      "The lower profile is valid for year 2050: True\n",
      "The flexibility is valid for year 2050: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# as the data was rounded, we check that the flexibility is lower or equal to the sum per day + epsilon\n",
    "epsilon = 5\n",
    "for year in years:\n",
    "    upper = dfs['04_EV', year, 'EV_power_profiles_LV.csv'][dfs['04_EV', '2030', 'EV_power_profiles_LV.csv']['Profile_type'] == 'Upper'].iloc[:,2:].values\n",
    "    base = dfs['04_EV',year, 'EV_power_profiles_LV.csv'][dfs['04_EV', '2030', 'EV_power_profiles_LV.csv']['Profile_type'] == 'Base'].iloc[:,2:].values\n",
    "    lower = dfs['04_EV', year, 'EV_power_profiles_LV.csv'][dfs['04_EV', '2030', 'EV_power_profiles_LV.csv']['Profile_type'] == 'Lower'].iloc[:,2:].values\n",
    "    flexibility = dfs['04_EV', year, 'EV_flexible_energy_profiles_LV.csv'].iloc[:,1:].values\n",
    "    max_deviation_per_hour = np.maximum(np.abs(upper - base), np.abs(lower - base))\n",
    "    # we get sum per day, considering that the shape is (n_bfs, n_hours) with n_hours = 8760. We have to sum every 24 hours\n",
    "    max_deviation_per_day = max_deviation_per_hour.reshape(-1, 365, 24)\n",
    "    sum_per_day = max_deviation_per_day.sum(axis=2)\n",
    "    # we check that the flexible profiles are equal or lower than the maximum deviation per day\n",
    "    valid_flexibility = (flexibility <= sum_per_day + epsilon).all()\n",
    "    print(f'The upper profile is valid for year {year}: {(upper >= base).all()}')\n",
    "    print(f'The lower profile is valid for year {year}: {(lower <= base).all()}')\n",
    "    print(f'The flexibility is valid for year {year}: {valid_flexibility}', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
